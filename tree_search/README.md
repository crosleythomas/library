---
layout: default
title: Tree Search
---

# Tree Search

| Year | Title | Authors | Affiliation | Code | Scholar | Other |
| --- | --- | --- | --- | --- | --- |
| 1996 | [On-line Policy Improvement using Monte-Carlo Search](papers/online_policy_improvement.pdf "We present a Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller. In the Monte-Carlo simulation, the long-term expected reward of each possible action is statistically measured, using the initial policy to make decisions in each step of the simulation. The action maximizing the measured expected reward is then taken, resulting in an improved policy. Our algorithm is easily parallelizable and has been implemented on the IBM SP! and SP2 parallel-RISC supercomputers. We have obtained promising initial results in applying this algorithm to the domain of backgammon. Results are reported for a wide variety of initial policies, ranging from a random policy to TD-Gammon, an extremely strong multi-layer neural network. In each case, the Monte-Carlo algorithm gives a substantial reduction, by as much as a factor of 5 or more, in the error rate of the base players. The algorithm is also potentially useful in many other adaptive control applications in which it is possible to simulate the environment.") | Gerald Tesauro, Gregory R. Galperin | IBM/MIT | | [Scholar](https://www.semanticscholar.org/paper/On-line-Policy-Improvement-using-Monte-Carlo-Searc-Tesauro-Galperin/3552fba431aa866bf9de293bebf7eff168e9e19c) | |
| March 2012 | [A Survey of Monte Carlo Tree Search Methods](papers/mcts_survey.pdf "Monte Carlo Tree Search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithmâ€™s derivation, impart some structure on the many variations and enhancements that have been proposed, and summarise the results from the key game and non-game domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.") | Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, Simon Colton | Imperial/Essex/Bradford | | [Scholar](https://www.semanticscholar.org/paper/A-Survey-of-Monte-Carlo-Tree-Search-Methods-Browne-Powley/0e2c4ad06ec462a961f195492941bc70afd560ae) | |
| Dec 2017 | [On Monte Carlo Tree Search and Reinforcement Learning](papers/on_mcts_and_rl.pdf "Fuelled by successes in Computer Go, Monte Carlo tree search (MCTS) has achieved widespread adoption within the games community. Its links to traditional reinforcement learning (RL) methods have been outlined in the past; however, the use of RL techniques within tree search has not been thoroughly studied yet. In this paper we re-examine in depth this close relation between the two fields; our goal is to improve the cross-awareness between the two communities. We show that a straightforward adaptation of RL semantics within tree search can lead to a wealth of new algorithms, for which the traditional MCTS is only one of the variants. We confirm that planning methods inspired by RL in conjunction with online search demonstrate encouraging results on several classic board games and in arcade video game competitions, where our algorithm recently ranked first. Our study promotes a unified view of learning, planning, and search.") | Tom Vodopivec, Spyridon Samothrakis, Branko Ster | Ljubljana/Essex | | | |